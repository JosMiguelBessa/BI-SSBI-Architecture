::Definir a fonte específica dos eventos a serem processados pelo logstash
input 
{
	::Neste caso, o input de dados é feito através de um ficheiro de texto (.txt)
	file
	{
		::Caminho para o ficheiro a processar
		path => "/opt/logstash/bin/dns_logs/query.log.1"
		::Definir o tipo de dados dos eventos processados no ficheiro de input
		type => "dnslogs"
		::Definir por onde começa a leitura do ficheiro (do início ou do fim)
		start_position => "beginning"
		::Verifica se o ficheiro de input já foi processado. Ao definir ignore_older => 0 se o ficheiro já tiver sido processado, não é processado novamente
		ignore_older => 0
		::Caminho para o ficheiro que faz o tracking dos eventos que já foram processados do ficheiro de input
		sincedb_path => "/opt/logstash/bin/indexes_since_db_track/producaodns"
	} 
}
::Bloco relativo à filtragem dos eventos presentes no ficheiro de input
filter 
{
	::grok é uma pattern que permite modular eventos do ficheiro de input (ideal para eventos provenientes de syslog, apache, mysql, webservers)
	grok
	{
		::Em cada iteração o campo message é populado pelos eventos do ficheiro de input	
		::match é utilizado para identificar os diferentes campos que constituem os eventos do ficheiro de input (presentes no campo message)
		match => ["message","((%{SYSLOGTIMESTAMP:datelog})\s*queries:\s*client\s*(%{IPV4:dns_client_ip})#(%{NONNEGINT:dns_uuid}):\s*query:\s*(%{USERNAME:dns_dest})\s*IN\s*(%{WORD:dns_record}))"]
	}

	::Mutate permite fazer alterações a variáveis (criar novos campos, apagar campos, mudar tipo de dados de campos)
	mutate
	{
		::add_field é utilizado para criar um novo campo
		add_field => ["year_log", "2014"]
	}

	mutate
	{
		add_field => ["created_datalog", "%{year_log} %{datelog}"]
	}

	::date é utilizado para definir campos do tipo data
	date
	{
		::Atribuir um formato de data ao campo com os dados referentes à data dos eventos do ficheiro de input
		match => ["created_datalog", "YYYY MMM dd HH:mm:ss.SSS", "YYYY MMM d HH:mm:ss.SSS"]
		::Definir a variável do tipo data que será usada no futuro para filtrar eventos num espaço temporal
		target => "@created_datalog"
		::Definir timezone da data dos eventos do ficheiro de input
		timezone => "UTC"
	}

	date
	{
		match => ["created_datalog", "YYYY MMM dd HH:mm:ss.SSS", "YYYY MMM d HH:mm:ss.SSS"]
		target => "@timestamp"
		timezone => "UTC"
	}

	::Todos os ficheiros que serão processados foram populados no final com a palavra END 
	::Assim que seja encontrado um evento apenas com a palavra END, termina o processamento
	if "END" in [message]
	{
		mutate
		{
			::adicionar a seguinte tag caso esteja terminado o processamento do ficheiro de input
			add_tag => ["end"]
			::remover a tag que é criada quando o processamento de eventos dá erro (evento não corresponde com o filtro grok)
			remove_tag => ["_grokparsefailure"]
			::remover a tag que é criada quando o processamento de eventos dá erro (formato da data do evento não corresponde ao formato de data definido no filtro)
			remove_tag => ["_dateparsefailure"]
		}
	}
}
::Definir o destino para onde será enviado o resultado do processamento dos eventos (documento) 
::presentes no ficheiro de input (Parte final da pipeline dos eventos)
output 
{
	::Caso exista a tag "end"
	if "end" in [tags]
	{
		::exec para executar um bash script
		::Neste caso o bash script irá avisar na linha de comandos quando o processamento de eventos estiver concluído
		exec
		{
			::dentro do exec é utilizado o command que leva como parametro uma string. Para que o ficheiro seja executado, devera ser colocado o tipo do ficheiro 
			::a executar antes do caminho do ficheiro
			command => "sh /opt/logstash/bin/stop_filter.sh"
		}
	}
	::Caso existam as tags "_dateparsefailure" e "_grokparsefailure"
	else if "_dateparsefailure" and "_grokparsefailure" in [tags]
	{
		::O destino do documento será uma base de dados Elasticsearch
		elasticsearch
		{
			::Utilizado para especificar a ação a tomar sobre a base de dados Elasticsearch 
			::Neste caso será feita uma indexação de documentos
			action => "index"
			::Utilizado para especificar o host/hosts onde se encontra a base de dados onde será armazenado o documento
			hosts => "localhost"
			::Utilizado para especificar a base de dados onde o documento será indexado
			index => "dnsserror"
		}
	}
	
	else
	{
		elasticsearch
		{
			action => "index"
			hosts => "localhost"
			index => "dns-%{+MM.YYYY}"
			::Permite especificar o caminho para o ficheiro onde é definido um template para os campos do documento a indexar 
			::(definir tipos de dados, se são analizados antes de indexar,...)
			template => ["/opt/logstash/bin/dnstemplate.json"]
			::Utilizado para validar um template customizado para o documento a indexar
			manage_template => true
			::Permite especificar que o template definido anteriormente será sobreposto a qualquer outro
			template_overwrite => true
		}
	}
	
	::Utilizado para imprimir na consola o processamento dos eventos do ficheiro de input, para que o utilizador possa fazer
	::o acompanhamento visual da execução do processo
	stdout
	{
		codec => rubydebug
	}
}
